<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>UIUC CS443 Reinforcement Learning - Course Note - Research Notes | Siyang Liu | University of Michigan</title>
    
    <!-- SEO Meta Tags -->
    <meta name="description" content="I have had some knowledge in RL but forgot too much after the graduation of my master, so I followed the materials in the below link to catch up." />
    <meta name="keywords" content="research notes, academic analysis, literature review, academic research, artificial intelligence, machine learning" />
    <meta name="author" content="Siyang Liu" />
    <meta name="robots" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1" />
    <meta name="language" content="en" />
    <meta name="revisit-after" content="7 days" />
    <meta name="distribution" content="global" />
    <meta name="rating" content="general" />
    
    <!-- Open Graph Meta Tags for Social Media -->
    <meta property="og:title" content="UIUC CS443 Reinforcement Learning - Course Note - Research Notes | Siyang Liu" />
    <meta property="og:description" content="I have had some knowledge in RL but forgot too much after the graduation of my master, so I followed the materials in the below link to catch up." />
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://lsy641.github.io/notes/uiuc-cs443-reinforcement-learning---course-note" />
    <meta property="og:image" content="https://lsy641.github.io/images/profile.jpg" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:image:alt" content="UIUC CS443 Reinforcement Learning - Course Note Research Notes" />
    <meta property="og:site_name" content="Siyang Liu - Academic Website" />
    <meta property="og:locale" content="en_US" />
    <meta property="article:author" content="Siyang Liu" />
    <meta property="article:published_time" content="2025-08-21T20:27:00+00:00" />
    <meta property="article:modified_time" content="2025-08-21T20:27:00+00:00" />
    <meta property="article:section" content="Research Notes" />
    <meta property="article:tag" content="['research notes', 'academic analysis', 'literature review']" />
    
    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="UIUC CS443 Reinforcement Learning - Course Note" />
    <meta name="twitter:description" content="I have had some knowledge in RL but forgot too much after the graduation of my master, so I followed..." />
    <meta name="twitter:image" content="https://lsy641.github.io/images/profile.jpg" />
    <meta name="twitter:creator" content="@liusiyang_641" />
    
    <!-- Canonical URL -->
    <link rel="canonical" href="https://lsy641.github.io/notes/uiuc-cs443-reinforcement-learning---course-note" />
    
    <!-- Academic Profile Links -->
    <link rel="author" href="https://scholar.google.com/citations?user=2OjUAPUAAAAJ" />
    
    <!-- Navigation Links -->
    <link rel="up" href="https://lsy641.github.io/research-notes" />
    <link rel="home" href="https://lsy641.github.io/" />
    
    <!-- Structured Data for Article -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "Article",
        "headline": "UIUC CS443 Reinforcement Learning - Course Note",
        "description": "I have had some knowledge in RL but forgot too much after the graduation of my master, so I followed the materials in the below link to catch up.",
        "image": "https://lsy641.github.io/images/profile.jpg",
        "author": {
            "@type": "Person",
            "name": "Siyang Liu",
            "url": "https://lsy641.github.io/",
            "jobTitle": "Ph.D. Student in Computer Engineering",
            "worksFor": {
                "@type": "Organization",
                "name": "University of Michigan"
            },
            "sameAs": [
                "https://scholar.google.com/citations?user=2OjUAPUAAAAJ"
            ]
        },
        "publisher": {
            "@type": "Organization",
            "name": "Siyang Liu's Academic Website",
            "url": "https://lsy641.github.io/"
        },
        "datePublished": "2025-08-21T20:27:00+00:00",
        "dateModified": "2025-08-21T20:27:00+00:00",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https://lsy641.github.io/notes/uiuc-cs443-reinforcement-learning---course-note"
        },
        "about": [
            {"@type": "Thing", "name": "Research Notes"}, 
            {"@type": "Thing", "name": "Academic Analysis"},
            {"@type": "Thing", "name": "Literature Review"}
        ],
        "keywords": "research notes, academic analysis, literature review, academic research, artificial intelligence, machine learning",
        "articleSection": "Research Notes",
        "inLanguage": "en",
        "isPartOf": {
            "@type": "CollectionPage",
            "name": "Research Notes",
            "url": "https://lsy641.github.io/research-notes"
        }
        
    }
    </script>
    
    <!-- Additional Structured Data for Breadcrumb -->
    <script type="application/ld+json">
    {
        "@context": "https://schema.org",
        "@type": "BreadcrumbList",
        "itemListElement": [
            {
                "@type": "ListItem",
                "position": 1,
                "name": "Home",
                "item": "https://lsy641.github.io/"
            },
            {
                "@type": "ListItem",
                "position": 2,
                "name": "Research Notes",
                "item": "https://lsy641.github.io/research-notes"
            },
            {
                "@type": "ListItem",
                "position": 3,
                "name": "Reading Notes",
                "item": "https://lsy641.github.io/notes/"
            }
        ]
    }
    </script>
    
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
            color: #333;
            background-color: #f8f9fa;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        h1 { font-size: 2.2em; border-bottom: 2px solid #3498db; padding-bottom: 10px; }
        h2 { font-size: 1.8em; border-bottom: 1px solid #bdc3c7; padding-bottom: 5px; }
        h3 { font-size: 1.4em; }
        h4 { font-size: 1.2em; }
        p { margin-bottom: 15px; }
        ul, ol { margin-bottom: 15px; padding-left: 30px; }
        li { margin-bottom: 5px; }
        
        /* Nested list styling */
        ul ul, ol ul, ul ol, ol ol {
            margin-top: 10px;
            margin-bottom: 10px;
            padding-left: 20px;
        }
        
        /* Make nested lists visually distinct */
        li > ul, li > ol {
            margin-top: 8px;
            margin-bottom: 8px;
        }
        
        /* Style for list items with nested content */
        li:has(ul), li:has(ol) {
            margin-bottom: 15px;
        }
        code {
            background-color: #f8f9fa;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            border-left: 4px solid #3498db;
        }
        pre code {
            background: none;
            padding: 0;
        }
        blockquote {
            border-left: 4px solid #3498db;
            margin: 20px 0;
            padding-left: 20px;
            color: #555;
            font-style: italic;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        strong { font-weight: bold; }
        em { font-style: italic; }
        hr {
            border: none;
            border-top: 1px solid #bdc3c7;
            margin: 30px 0;
        }
        .paper-meta {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border-left: 4px solid #3498db;
        }
        .note-info {
            background-color: #e8f4fd;
            border-left: 4px solid #3498db;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        .warning {
            background-color: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
            border-radius: 0 5px 5px 0;
        }
        .breadcrumb {
            background-color: #f8f9fa;
            padding: 10px 20px;
            border-radius: 5px;
            margin-bottom: 0px;
            font-size: 0.9em;
        }
        .breadcrumb a {
            color: #666;
        }
        .breadcrumb a:hover {
            color: #3498db;
        }
        .navigation {
            margin: 0 0;
            padding: 5px;
            background-color: #f8f9fa;
            border-radius: 8px;
            border: 0px solid #e9ecef;
        }
        .navigation a {
            margin-right: 15px;
            padding: 8px 15px;
            background-color: #3498db;
            color: white;
            border-radius: 5px;
            text-decoration: none;
            font-weight: 500;
            transition: background-color 0.2s ease;
        }
        .navigation a:hover {
            background-color: #2980b9;
            transform: translateY(-1px);
        }
        article {
            background: white;
            padding: 20px;
            border-radius: 12px;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            width: 100%;
            margin: 20px 0;
        }
        header {
            margin-bottom: 30px;
        }
        footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #eee;
            font-size: 0.9em;
            color: #666;
        }
    </style>
    
</head>
<body>
    <!-- Breadcrumb Navigation -->
    <nav class="breadcrumb" aria-label="Breadcrumb">
        <a href="https://lsy641.github.io/">Home</a> &gt; 
        <a href="https://lsy641.github.io/research-notes">Research Notes</a> &gt; 
        Reading Notes
    </nav>

    <!-- Navigation Links -->
    <div class="navigation">
        <a href="https://lsy641.github.io/">← Back to Home</a>
        <a href="https://lsy641.github.io/research-notes">← Back to Research Notes</a>
    </div>

    <article>
        <h2>UIUC CS443 Reinforcement Learning - Course Note</h2>
        
        


<p>I have had some knowledge in RL but forgot too much after the graduation of my master, so I followed the materials in the below link to catch up.</p>

<a href="https://nanjiang.cs.illinois.edu/cs443/" rel="noopener" target="_blank">Course Link</a>

<h2>C1: Introduction</h2>

<h3>Markov Decision Process</h3>

<p>A Markov decision process is a 4-tuple $(S,A,P_{a},R_{a})$
$S$: state
$A$: action
$P_a$: transition. $P(S_(t+1)|S_t, a_t)$ (So action has control over transition)
$R_{a}$: immediate reward $R_s^a = E[R_(t+1)|S_t=s, A_t=a]$
MDP assumes the future depends on the present and not the past.
Q1: but the current state can be represented using the past information?
Q2: why the reward function is designed as expectation that thus is independent with the past? Can it be a matrix $R(S_(t+1),S_t, A_t)$? Using expectation will ignore an optimal path to {S_t} from some {S_(t-1)^k} though rewards from {S_{t-1}^!k} are extreme low. This make the game not suited for achieving highest rewards (with high risk) but optimal reward expectation.</p>

<h3>Optimal Policy</h3>

<p>Optimal policy $\pi$ guides how the agent acts in its current state.
What is optimal needs to be defined. Maximize expected "discounted future reward sum" (return) is an objective for getting optimal policy.</p>

<h3>Core Challenges of RL</h3>

<ol>
<li>Temporal credit assignment A sequence of actions led to success/failure: which action(s) to attribute the consequence to?
</li>
</ol>
<p>A sequence of actions led to success/failure: which action(s) to attribute the consequence to?</p>

<ol>
<li>Exploration How to take actions to collect a dataset that provides a comprehensive description of the environment?
</li>
</ol>
<p>How to take actions to collect a dataset that provides a comprehensive description of the environment?</p>

<ol>
<li>Generalization How to deal with (very) large state spaces?
</li>
</ol>
<p>How to deal with (very) large state spaces?</p>

<h3>A Machine Learning view of RL</h3>

<p>contextual bandits take into account specific user or environment characteristics (context) when choosing an action.
bandits focus on immediate rewards in a single-step scenario.</p>

<h3>A few misconceptions about RL</h3>

<p>RL: Planning or Learning??
Two types of scenarios in RL research: 1. Solving a large planning problem using a learning approach (e.g., alphago). Transition dynamics (Go rules) known, but too many states. Run the simulator to collect data. Get good value funtions.</p>
<ol>
<li>Solving a learning problem (e.g.,  adaptive medical treatment). Transition dynamics unknown (and too many states). Interact with the environment to collect data. Great potential for RL, but not realized yet!
</li>
</ol>

<p>Is RL the magical blackbox in machine learning?</p>

<ol>
<li>Contextual bandit is an intermediate framework between SL & RL.
</li>
<li>General rule: more flexibility/generality = less tractability!
</li>
<li>So, RL should be your last resort when the problem cannot be handled by any simpler framework.
</li>
<li>Be cautious about reducing an SL problem to RL—some are legit but many fall into the trap of reducing a simple problem to a more difficult one
</li>
</ol>

<h2>C2: Markov Decision Processes</h2>


<p>An MDP $M = (A, S, P, R, \gamma) $
If there is only one action, MDP collaps into Markov Chain with a reward function
If the transition is deterministic, the MDP becomes a directed graph</p>

<h3>Why discounting? (cont.)</h3>

<p>Even very light discount make the expected reward finite.
Heavy discount encourages fast planning / learning.
Return: $G_t = \sum_i^N \gamma^k R_{t+1+i}$. A trajectory corresponds with a return. With policy distribution, we can get expected return (value function).
With return we can have state-value function $E_{pi}[G_t|S_t=s]$, action-value or q-value function $E[G_t|S_t=s, A_t=a]$
Adavantage function: difference between state-value and action-value
$E_{\pi}[G_t|A_t=a, S_t=s] - E_{\pi}[G_t|S_t=s]$
Bellman Equation provides recursive formula for calculating value function. Because reward is unrelated to previous states. The recursive equation has a direct solution. $V_{\pi} = R_s + \gamma \sum P_{s}V_{\pi}$
To solve optimal value function, if we know reward and transition probability function, we can solve it via DP, if not, there is no close form solution. We can use Monte-Carlo methods, Temporal difference learning (model-free and learns with episodes), Policy gradient (REINFORCE, Actor-Critic, A2C/A3C, ACKTR, PPO, DPG, DDPG).
Model-free means we don't know transition probability.
Q: again, will using expectation cause an expected best (smooth) but not best policy?
Q: similar to the advantage function that consider a factor (action), can we consider other factors such time, so we get difference between state-value functions from the same state but different time or action-value functions of it.
Q: the problem and its mathematical definition have a lot of assumptions... why can't we design in another way?</p>



        <footer>
            <hr>
            <p><em>Notes by Siyang Liu - Last updated: August 21, 2025</em></p>
            <p><strong>Author:</strong> <a href="https://lsy641.github.io/">Siyang Liu</a> | <strong>Google Scholar:</strong> <a href="https://scholar.google.com/citations?user=2OjUAPUAAAAJ" rel="noopener" target="_blank">Profile</a></p>
        </footer>
    </article>
</body>
</html>